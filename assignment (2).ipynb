{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AarKeibkccil"
      },
      "source": [
        "Name:       Betelihem Mengist\n",
        "\n",
        "ID:         UGR/4777/12\n",
        "\n",
        "Stream:     Software\n",
        "\n",
        "Section:    1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tjsJ5Q2KXR8",
        "outputId": "5b3dbd01-07ec-4933-d4b0-c40b6cbd080b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to /content/drive/\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3v4bk6nMIiU"
      },
      "outputs": [],
      "source": [
        "# Path to my file on Google Drive\n",
        "file_path = '/content/drive/My Drive/GPAC.txt'\n",
        "\n",
        "# Now I can read the file and perform operations\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIk5opRcfx9j"
      },
      "source": [
        "1.1_Create n-grams for n=1, 2, 3, 4. You can show sample prints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9S7CGRdsyzf",
        "outputId": "01a13bc2-e468-4de1-9e80-424d90bf2693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-grams:\n",
            "ምን\n",
            "መሰላችሁ?\n",
            "(አንባቢያን)\n",
            "ኢትዮጵያ\n",
            "በተደጋጋሚ\n",
            "ጥሪው\n",
            "ደርሷት\n",
            "ልትታደመው\n",
            "ያልቻለችው\n",
            "የአለም\n",
            "የእግር\n",
            "ኳስ\n",
            "ዋ\n",
            "ለ19ኛ\n",
            "ጊዜ\n",
            "በደቡብ\n",
            "አፍሪካ\n",
            "ሲጠጣ፣\n",
            "በሩቅ\n",
            "\n",
            "2-grams:\n",
            "ምን መሰላችሁ?\n",
            "መሰላችሁ? (አንባቢያን)\n",
            "(አንባቢያን) ኢትዮጵያ\n",
            "ኢትዮጵያ በተደጋጋሚ\n",
            "በተደጋጋሚ ጥሪው\n",
            "ጥሪው ደርሷት\n",
            "ደርሷት ልትታደመው\n",
            "ልትታደመው ያልቻለችው\n",
            "ያልቻለችው የአለም\n",
            "የአለም የእግር\n",
            "የእግር ኳስ\n",
            "ኳስ ዋ\n",
            "ዋ ለ19ኛ\n",
            "ለ19ኛ ጊዜ\n",
            "ጊዜ በደቡብ\n",
            "በደቡብ አፍሪካ\n",
            "አፍሪካ ሲጠጣ፣\n",
            "ሲጠጣ፣ በሩቅ\n",
            "\n",
            "3-grams:\n",
            "ምን መሰላችሁ? (አንባቢያን)\n",
            "መሰላችሁ? (አንባቢያን) ኢትዮጵያ\n",
            "(አንባቢያን) ኢትዮጵያ በተደጋጋሚ\n",
            "ኢትዮጵያ በተደጋጋሚ ጥሪው\n",
            "በተደጋጋሚ ጥሪው ደርሷት\n",
            "ጥሪው ደርሷት ልትታደመው\n",
            "ደርሷት ልትታደመው ያልቻለችው\n",
            "ልትታደመው ያልቻለችው የአለም\n",
            "ያልቻለችው የአለም የእግር\n",
            "የአለም የእግር ኳስ\n",
            "የእግር ኳስ ዋ\n",
            "ኳስ ዋ ለ19ኛ\n",
            "ዋ ለ19ኛ ጊዜ\n",
            "ለ19ኛ ጊዜ በደቡብ\n",
            "ጊዜ በደቡብ አፍሪካ\n",
            "በደቡብ አፍሪካ ሲጠጣ፣\n",
            "አፍሪካ ሲጠጣ፣ በሩቅ\n",
            "\n",
            "4-grams:\n",
            "ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ\n",
            "መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ\n",
            "(አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው\n",
            "ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት\n",
            "በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\n",
            "ጥሪው ደርሷት ልትታደመው ያልቻለችው\n",
            "ደርሷት ልትታደመው ያልቻለችው የአለም\n",
            "ልትታደመው ያልቻለችው የአለም የእግር\n",
            "ያልቻለችው የአለም የእግር ኳስ\n",
            "የአለም የእግር ኳስ ዋ\n",
            "የእግር ኳስ ዋ ለ19ኛ\n",
            "ኳስ ዋ ለ19ኛ ጊዜ\n",
            "ዋ ለ19ኛ ጊዜ በደቡብ\n",
            "ለ19ኛ ጊዜ በደቡብ አፍሪካ\n",
            "ጊዜ በደቡብ አፍሪካ ሲጠጣ፣\n",
            "በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the first 100 characters from the file\n",
        "file_path = '/content/drive/My Drive/GPAC.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read(100)\n",
        "\n",
        "# Define a function to generate n-grams\n",
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    ngrams = []\n",
        "\n",
        "    for i in range(len(words) - n + 1):\n",
        "        ngram = ' '.join(words[i:i + n])\n",
        "        ngrams.append(ngram)\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "# Generate and print n-grams for (1-4)\n",
        "for n in [1, 2, 3, 4]:\n",
        "    ngrams = generate_ngrams(text, n)\n",
        "    print(f'{n}-grams:')\n",
        "    for ngram in ngrams:\n",
        "        print(ngram)\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qST_FscgCjh"
      },
      "source": [
        "*1.2_Calculate* probabilities of n-grams and find the top 10 most likely n-grams for all n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpBFTeG4k8Dg",
        "outputId": "eea47cce-90d4-4b66-a1d4-8b9d782bf6c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-grams:\n",
            "Top 10 most likely:\n",
            "እና: 0.0105\n",
            "ነው፡፡: 0.0089\n",
            "ወደ: 0.0082\n",
            "እንደ: 0.0080\n",
            "ላይ: 0.0080\n",
            "ነው: 0.0074\n",
            "ውስጥ: 0.0058\n",
            "ግን: 0.0053\n",
            "አንድ: 0.0047\n",
            "ሳሚ: 0.0038\n",
            "\n",
            "2-grams:\n",
            "Top 10 most likely:\n",
            "ቢግ ብራዘርስ: 0.0018\n",
            "ቢግ ብራዘር: 0.0015\n",
            "ደቡብ አፍሪካ: 0.0009\n",
            "ብራዘርስ አፍሪካ: 0.0009\n",
            "እንደ አዲስ: 0.0009\n",
            "ምናብ ውስጥ: 0.0009\n",
            "አብሮ የመኖር: 0.0007\n",
            "ከቢግ ብራዘር: 0.0007\n",
            "ውጤት ነው፡፡: 0.0007\n",
            "እንደ ሰካራም: 0.0007\n",
            "\n",
            "3-grams:\n",
            "Top 10 most likely:\n",
            "ቢግ ብራዘርስ አፍሪካ: 0.0007\n",
            "እ ኤ አ: 0.0007\n",
            "ወደ ደቡብ አፍሪካ: 0.0004\n",
            "አብሮ የመኖር ውድድር: 0.0004\n",
            "ቢግ ብራዘርስ ኢትዮጵያ: 0.0004\n",
            "የእኛን ቢግ ብራዘር: 0.0004\n",
            "ሃኒ በ ቢግ: 0.0004\n",
            "ያገኘውን ነገር ሁሉ: 0.0004\n",
            "ቀኑን ሙሉ ሲመለከት: 0.0004\n",
            "አቅጣጫ አራት ጊዜ: 0.0004\n",
            "\n",
            "4-grams:\n",
            "Top 10 most likely:\n",
            "አቅጣጫ አራት ጊዜ አራት: 0.0004\n",
            "አራት ጊዜ አራት በሆነ: 0.0004\n",
            "ጊዜ አራት በሆነ የእጅ: 0.0004\n",
            "አራት በሆነ የእጅ ብዛት: 0.0004\n",
            "አንድ ላይ ተጨባብጠው፣ ተቆላልፈው: 0.0004\n",
            "ላይ ተጨባብጠው፣ ተቆላልፈው ፡፡: 0.0004\n",
            "ተጨባብጠው፣ ተቆላልፈው ፡፡ እጆቹ: 0.0004\n",
            "ተቆላልፈው ፡፡ እጆቹ ተለያይተው: 0.0004\n",
            "እንደ አዲስ ከጭረት መጀመር: 0.0004\n",
            "አንድ ሚሊዮን ዶላር ቢኖረኝ: 0.0004\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define a function to generate n-grams\n",
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    ngrams = []\n",
        "\n",
        "    for i in range(len(words) - n + 1):\n",
        "        ngram = ' '.join(words[i:i + n])\n",
        "        ngrams.append(ngram)\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "# Read the first 30000 characters from the file\n",
        "with open('/content/drive/My Drive/GPAC.txt', 'r', encoding='utf-8') as file:\n",
        "    text_sample = file.read(30000)\n",
        "\n",
        "# Generate and print n-grams for different values of n\n",
        "for n in [1, 2, 3, 4]:\n",
        "    ngrams = generate_ngrams(text_sample, n)\n",
        "\n",
        "    # Calculate n-gram probabilities\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    total_ngrams = len(ngrams)\n",
        "    probabilities = {k: v / total_ngrams for k, v in ngram_counts.items()}\n",
        "\n",
        "    # Find the top 10 most likely n-grams\n",
        "    top_10_ngrams = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    # Print results\n",
        "    print(f'{n}-grams:')\n",
        "    print(\"Top 10 most likely:\")\n",
        "    for ngram, probability in top_10_ngrams:\n",
        "        print(f\"{ngram}: {probability:.4f}\")\n",
        "\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCxrX0PAiDae"
      },
      "source": [
        "1.3_What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3kq54lXsKyO",
        "outputId": "0687f45d-34b3-472a-e25e-781b7cbf29f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of the sentence 'ኢትዮጵያ ታሪካዊ ሀገር ናት' is: 0.0000000000\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
        "\n",
        "def calculate_ngram_probabilities(text, n):\n",
        "    ngrams = generate_ngrams(text, n)\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    total_ngrams = len(ngrams)\n",
        "    probabilities = {k: v / total_ngrams for k, v in ngram_counts.items()}\n",
        "    return probabilities\n",
        "\n",
        "def calculate_sentence_probability(sentence, ngram_probabilities):\n",
        "    sentence_ngrams = generate_ngrams(sentence, len(list(ngram_probabilities.keys())[0].split()))\n",
        "    sentence_probability = 1.0\n",
        "    for ngram in sentence_ngrams:\n",
        "        sentence_probability *= ngram_probabilities.get(ngram, 0.0)\n",
        "    return sentence_probability\n",
        "\n",
        "# Read the first 100000000 characters from the file\n",
        "with open('/content/drive/My Drive/GPAC.txt', 'r', encoding='utf-8') as file:\n",
        "    text_sample = file.read(100000000)\n",
        "\n",
        "# Calculate n-gram probabilities for a smaller subset (first 100 characters)\n",
        "ngram_probabilities = calculate_ngram_probabilities(text_sample, 4)\n",
        "\n",
        "# Given sentence\n",
        "sentence = \"ኢትዮጵያ ታሪካዊ ሀገር ናት\"\n",
        "\n",
        "# Calculate the probability of the sentence using the precomputed n-gram probabilities\n",
        "sentence_probability = calculate_sentence_probability(sentence, ngram_probabilities)\n",
        "\n",
        "print(f\"The probability of the sentence '{sentence}' is: {sentence_probability:.10f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8YWm0Fjsf8t"
      },
      "source": [
        "You can also try more sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcE8KofjrScj",
        "outputId": "c4d0059a-b495-44c2-b63c-69fa1e979b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of the sentence 'አብሮ የመኖር ውድድር ' is: 1.0000000000\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
        "\n",
        "def calculate_ngram_probabilities(text, n):\n",
        "    ngrams = generate_ngrams(text, n)\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    total_ngrams = len(ngrams)\n",
        "    probabilities = {k: v / total_ngrams for k, v in ngram_counts.items()}\n",
        "    return probabilities\n",
        "\n",
        "def calculate_sentence_probability(sentence, ngram_probabilities):\n",
        "    sentence_ngrams = generate_ngrams(sentence, len(list(ngram_probabilities.keys())[0].split()))\n",
        "    sentence_probability = 1.0\n",
        "    for ngram in sentence_ngrams:\n",
        "        sentence_probability *= ngram_probabilities.get(ngram, 0.0)\n",
        "    return sentence_probability\n",
        "\n",
        "# Read the first 100000000 characters from the file\n",
        "with open('/content/drive/My Drive/GPAC.txt', 'r', encoding='utf-8') as file:\n",
        "    text_sample = file.read(10000000)\n",
        "\n",
        "# Calculate n-gram probabilities for a smaller subset (first 100 characters)\n",
        "ngram_probabilities = calculate_ngram_probabilities(text_sample, 4)\n",
        "\n",
        "# Given sentence\n",
        "sentence = \"አብሮ የመኖር ውድድር \"\n",
        "\n",
        "# Calculate the probability of the sentence using the precomputed n-gram probabilities\n",
        "sentence_probability = calculate_sentence_probability(sentence, ngram_probabilities)\n",
        "\n",
        "print(f\"The probability of the sentence '{sentence}' is: {sentence_probability:.10f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1mBwprliv73"
      },
      "source": [
        "1.4_Generate random sentences using n-grams; explain what happens as n-increases\n",
        "based on your output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2jt4mFN986-",
        "outputId": "1e95dcb9-b671-48fc-edc3-6ed202185e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated random sentence for n=1:\n",
            "ስንፈልገው የነበረ ነው፡፡ የነበረ ነው፡፡ ኢኒስትራክተር ነው፡፡ ኢኒስትራክተር ተመልካቹን ኢኒስትራክተር ተመልካቹን የምንጠብቅበት\n",
            "\n",
            "Generated random sentence for n=2:\n",
            "ሊለወጡ እንደሚችሉ እና እንደሚችሉ እና ምድር እና ምድር የሚፈልጉትን ምድር የሚፈልጉትን ልቦለድ\n",
            "\n",
            "Generated random sentence for n=3:\n",
            "የሚገል በሚል ቢያወግዙትም በሚል ቢያወግዙትም ተጓትቷል፡፡ ቢያወግዙትም ተጓትቷል፡፡ (ባለስልጣናትን) ተጓትቷል፡፡ (ባለስልጣናትን) ይጥሉን\n",
            "\n",
            "Generated random sentence for n=4:\n",
            "ባቱታን አስቆጥተውታል፡፡ ቆርጦ አስቆጥተውታል፡፡ ቆርጦ የሦስቱም ቆርጦ የሦስቱም ዓመት የሦስቱም ዓመት ባለመስራት\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_random_sentence(ngram_probabilities, max_length=20):\n",
        "    sentence = []\n",
        "    current_ngram = random.choice(list(ngram_probabilities.keys()))\n",
        "\n",
        "    while len(sentence) < max_length:\n",
        "        sentence.extend(current_ngram.split())\n",
        "        next_word = random.choices(\n",
        "            list(ngram_probabilities.keys()),\n",
        "            weights=list(ngram_probabilities.values())\n",
        "        )[0].split()[-1]\n",
        "        current_ngram = ' '.join(current_ngram.split()[1:] + [next_word])\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Read the first 1000000 characters from the file\n",
        "with open('/content/drive/My Drive/GPAC.txt', 'r', encoding='utf-8') as file:\n",
        "    text_sample = file.read(1000000)\n",
        "\n",
        "# Calculate n-gram probabilities for a smaller subset (first 100 characters)\n",
        "ngram_probabilities = calculate_ngram_probabilities(text_sample, 3)\n",
        "\n",
        "# Generate random sentences for different values of n\n",
        "for n in [1, 2, 3,4]:\n",
        "    print(f\"\\nGenerated random sentence for n={n}:\")\n",
        "    random_sentence = generate_random_sentence(ngram_probabilities, max_length=10)\n",
        "    print(random_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jz31kZiUmT_"
      },
      "source": [
        "As n increases, the model considers longer sequences of words for prediction, potentially capturing more context. However, increasing n also means that the model requires more training data, and the generated sentences become more rigid and context-specific. It's a trade-off between capturing long-range dependencies and maintaining flexibility in sentence generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz4BHS4MjIfS"
      },
      "source": [
        "#2 Evaluate these Language Models Using Intrinsic Evaluation Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDzTZ33z-Z7_",
        "outputId": "3b3e39da-6eee-4894-9a97-53f68f16bc52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n-1 Perplexity: inf\n",
            "n-2 Perplexity: inf\n",
            "n-3 Perplexity: inf\n",
            "n-4 Perplexity: inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download the necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Path to my file on Google Drive\n",
        "file_path = '/content/drive/My Drive/GPAC.txt'\n",
        "\n",
        "# Now I can read the first 100000000 characters from the file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read(1000)\n",
        "\n",
        "# Tokenize the text into sentences and words\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, valid_data = tokenized_text[:int(0.8 * len(tokenized_text))], tokenized_text[int(0.8 * len(tokenized_text)):]\n",
        "\n",
        "# Train a Maximum Likelihood Estimation (MLE) language model\n",
        "for n in [1, 2, 3, 4]:\n",
        "    train_ngrams, vocab = padded_everygram_pipeline(n, train_data)\n",
        "\n",
        "    # Convert vocab to nltk.lm.Vocabulary\n",
        "    from nltk.lm.vocabulary import Vocabulary\n",
        "    vocab = Vocabulary(vocab)\n",
        "\n",
        "    lm = MLE(order=n, vocabulary=vocab)\n",
        "    lm.fit(train_ngrams)\n",
        "\n",
        "    # Convert the iterator to a list for each sentence\n",
        "    valid_ngrams = [list(nltk.lm.preprocessing.pad_both_ends(sent, n=n)) for sent in valid_data]\n",
        "    perplexity = lm.perplexity(valid_ngrams)\n",
        "\n",
        "    # Print Evaluation Metrics\n",
        "    print(f\"n-{n} Perplexity: {perplexity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj5tJwlHkj4K"
      },
      "source": [
        "#3 Evaluate these Language Models Using Extrinsic Evaluation Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27vr6ehfb74R",
        "outputId": "69c7736a-c782-4f0e-ec9f-111c6affcfd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "\n",
        "# Load the content of GPAC.txt with specified encoding\n",
        "file_path = '/content/drive/My Drive/GPAC.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read(1000000)\n",
        "\n",
        "# Assume the text is organized into documents with corresponding labels (0 or 1)\n",
        "documents = [\"ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ\", \"በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው\"]\n",
        "labels = [0, 1]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        documents, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "# Vectorize the text data using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "# Train a Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, train_labels)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier and print the accuracy\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}